{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa567129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.2-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\josiah lee\\documents\\github\\dse4212_portfolio_optimisation\\.venv\\lib\\site-packages (from scikit-learn) (2.3.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\josiah lee\\documents\\github\\dse4212_portfolio_optimisation\\.venv\\lib\\site-packages (from scikit-learn) (1.16.2)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.7.2-cp313-cp313-win_amd64.whl (8.7 MB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "\n",
      "   ------------- -------------------------- 1/3 [joblib]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   ---------------------------------------- 3/3 [scikit-learn]\n",
      "\n",
      "Successfully installed joblib-1.5.2 scikit-learn-1.7.2 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdc743c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fredapi import Fred\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbaaef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_recent_series_of_date(series_key, end_date, fred):\n",
    "    '''\n",
    "    Retrieves FRED data on an economic indicator up till the latest entry as of a certain date.\n",
    "\n",
    "    Parameters:\n",
    "    ______________________________\n",
    "    series_key: string\n",
    "        The key of the economic indicator in FRED's database: https://fred.stlouisfed.org/release?rid=205.\n",
    "\n",
    "    end_date: string\n",
    "        The date in YYYY-MM-DD format. \n",
    "\n",
    "    fred: fredapi \n",
    "        The fredapi object to pull FRED data from.\n",
    "\n",
    "    Returns: \n",
    "    ______________________________\n",
    "    pandas.Series\n",
    "        The values of the input economic indicator, with a date index.\n",
    "    '''    \n",
    "    df = fred.get_series_as_of_date(series_key, end_date).drop_duplicates(subset = [\"date\"], keep = \"last\")\n",
    "    df = pd.Series(df[\"value\"].to_list(), index = df[\"date\"].to_list())\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df = df.dropna()\n",
    "    df = df.astype(\"float\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81a8e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "given_date = \"2007-12-01\"\n",
    "\n",
    "fred = Fred(api_key = os.getenv(\"API_KEY\"))\n",
    "df = get_most_recent_series_of_date(\"DTB3\", given_date, fred)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0166e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('all_stocks_5yr.csv', index_col=0, parse_dates=True)\n",
    "df = df.pivot_table(index='date', columns='Name', values='close').dropna(axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a425511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert close prices to daily returns\n",
    "returns = np.log(df).diff().dropna()\n",
    "\n",
    "# 2. Split into estimation (first 4 years) and test (last year)\n",
    "split_date = returns.index[0] + pd.DateOffset(years=4)\n",
    "returns_est = returns[returns.index < split_date]\n",
    "returns_test = returns[returns.index >= split_date]\n",
    "\n",
    "# 3. Estimate mean and covariance from estimation period\n",
    "mean_returns = returns_est.mean()\n",
    "cov_matrix = returns_est.cov()\n",
    "\n",
    "# 4. Find minimum variance portfolio weights\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def portfolio_variance(weights, cov_matrix):\n",
    "    return weights.T @ cov_matrix @ weights\n",
    "\n",
    "num_assets = len(mean_returns)\n",
    "constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
    "bounds = tuple((0, 1) for _ in range(num_assets))\n",
    "init_guess = np.repeat(1/num_assets, num_assets)\n",
    "\n",
    "result = minimize(portfolio_variance, init_guess, args=(cov_matrix,), \n",
    "                  method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "min_var_weights = result.x\n",
    "\n",
    "# 5. Scenario 1: No rebalancing (buy and hold)\n",
    "initial_weights = min_var_weights\n",
    "weight_tracker = [initial_weights.copy()]\n",
    "return_tracker = []\n",
    "\n",
    "for daily_returns in returns_test.values:\n",
    "    # Assets grow at different rates\n",
    "    return_tracker.append(daily_returns @ weight_tracker[-1])\n",
    "    new_weights = weight_tracker[-1] * (1 + daily_returns)\n",
    "    new_weights = new_weights / new_weights.sum()\n",
    "    weight_tracker.append(new_weights)\n",
    "\n",
    "return_tracker = pd.Series(return_tracker, index=returns_test.index)\n",
    "\n",
    "cum_returns = (return_tracker + 1).cumprod()\n",
    "sharpe_ratio = (np.mean(return_tracker) / np.std(return_tracker)) * np.sqrt(252)\n",
    "\n",
    "print(\"No Rebalancing Sharpe Ratio:\", sharpe_ratio)\n",
    "\n",
    "# 6. Scenario 2: Daily rebalancing with transaction fee (10 bps per turnover)\n",
    "fee_rate = 0.002  # 10 basis points to sell then buy\n",
    "target_weights = min_var_weights.copy()\n",
    "current_weights = target_weights.copy()\n",
    "portfolio_values = [1.0]\n",
    "daily_returns = []\n",
    "\n",
    "for i, row in returns_test.iterrows():\n",
    "    # Assets grow at their individual rates - weights drift naturally\n",
    "    weights_after_growth = current_weights * (1 + row)\n",
    "    \n",
    "    # Calculate turnover: sum of absolute trades needed to rebalance\n",
    "    turnover = np.sum(np.abs(target_weights*weights_after_growth.sum() - weights_after_growth))\n",
    "    \n",
    "    # Portfolio return before fees\n",
    "    port_ret = np.dot(current_weights, row)\n",
    "    \n",
    "    # Deduct transaction costs\n",
    "    port_ret_after_fees = port_ret - (turnover * fee_rate)\n",
    "    \n",
    "    # Update portfolio value\n",
    "    new_value = portfolio_values[-1] * (1 + port_ret_after_fees)\n",
    "    portfolio_values.append(new_value)\n",
    "    daily_returns.append(port_ret_after_fees)\n",
    "    \n",
    "    # Rebalance back to target weights for next period\n",
    "    current_weights = target_weights.copy()\n",
    "\n",
    "# Calculate Sharpe ratio correctly from returns, not cumulative values\n",
    "daily_returns = np.array(daily_returns)\n",
    "rebal_sharpe = (daily_returns.mean() / daily_returns.std()) * np.sqrt(252)\n",
    "\n",
    "print(\"Daily Rebalancing Sharpe Ratio (with fees):\", rebal_sharpe)\n",
    "\n",
    "# 7. Plot results\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(cum_returns.index, cum_returns, label='No Rebalancing')\n",
    "plt.plot(returns_test.index, portfolio_values[1:], label='Daily Rebalancing (with fees)')\n",
    "plt.legend()\n",
    "plt.title('Minimum Variance Portfolio Performance')\n",
    "plt.ylabel('Cumulative Return')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
